## Лекция 1

Беше уводна. Може да си погледнеш презентацията към нея.

## Лекция 2

Хранилище: https://github.com/iproduct/iagenai

Показваше ни как да правим графики и хистограми с Python.  
Ще ползваме IDE PyCharm, като няй-удобно.  
Говорихме за статистика:
- percentiles
- normal distribution
- дисперсия

Говорихме за нещата от курса по ИИ.
Заб. Всъщност нещата от курса по ИИ ще са
- Training and testing datasets; overfitting и как можем да го разрешим при различните алгоритми.
- Variance и bias в невронни мрежи (и не само при невронните мрежи). Балансът между двете.
- True positive vs false positive, true negative and false negative.
- Дефиницията на метриките accuracy, recall, precision, F1 score.
- Двоични класификатори.
- Рок крива за сравняване на модели.
- Confusion matrix - при класификация на примери (какво ни казва).
- Linear regression and polynomial regression - има в презентацията връзки към Wikipedia.
- Loss function - обща дефиниция. Различни видове loss функции.
- имаме примери в хранилището.
- root mean squared error
- r2 score - до колко нашият модел се справя по-добре от някакъв стандартен.
- различни метрики за оценка колко добре се справят алгоритмите с класификацията.
- визуализацията на данните с plot ще е важно да го науча.
- pandas и numpy ще са доста нужни.
- Distance measures - Евклидово, Манхатаново, Минковски са най-известните.
- Дефиницията на *норма* - определя разстоянието между два примера. Има и някакви условия, за да считаме,
  че е 'хубава' норма. Има линк в Мудъл за норми.

Да се науча да работя с Jupiter Notebook файлове.

## Лекция 3

Споменахме data preprocessing.  
Определено да си преговоря (т.е. науча) нещата за `pandas` и `numpy`.  
One hot encoding - да прочета.  
Logistic regression разгледахме.  

### K nearest neighbors.

- Добре е **k** да е нечетно (когато работим с 2 класа), защото иначе могат да се получат равенства.
- За да имаме k най-близки съседи ни трябва начин за измерване на разстоянието между примерите, т.е. 
  ни трябва *норма*.
- Особено чувствителен към голяма размерност на пространството (curse of dimensionality).
- Feature extraction.

### Clustering (клъстеризация).

- k-means clustering (center-based clustering).

### Feature selection

Какво подмножество от характеристики да вземем, така че най-добре да определят тестовите примери 
(най-лесно да различаваме различните примери). Разни подходи как да избираме характеристики.
Пример за добра характеристика, ако искаме да предвидим теглото на човек, е височината му. Лоша 
характеристика е например номера на личната му карта.

### Decision Trees 

Само ги споменахме.

### Support Vector Machines (SVM)

- Kernel trick